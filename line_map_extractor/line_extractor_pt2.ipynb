{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "line_extractor_pt2.py \n",
    "\n",
    "This script merges redudant 3D lines based on parallel and proximity conditions.\n",
    "You can tune the paramters defined in helper.py.\n",
    "\n",
    "Output:\n",
    "- Rgb images annotated with extracted lines thier semantic labels. \n",
    "- Mesh file(.ply) with all merged 3D lines.\n",
    "- One 3D line Mesh file(.ply) for each semantic label.\n",
    "- A numpy file containing all the extracted 2D lines and regressed 3D lines.\n",
    "\n",
    "Author: Haodong JIANG <221049033@link.cuhk.edu.cn>\n",
    "Version: 1.0\n",
    "License: MIT\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import os\n",
    "import open3d as o3d\n",
    "# from collections import Counter\n",
    "from joblib import Parallel, delayed\n",
    "from scipy import stats\n",
    "from tqdm import tqdm\n",
    "import helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from pt1\n",
    "scene_list = [\"69e5939669\",\"689fec23D7\",\"c173f62b15\",\"55b2bf8036\"]\n",
    "scene_id = scene_list[2]\n",
    "### load data path\n",
    "root_dir = \"/data1/home/lucky/ELSED\"\n",
    "scene_data_path = root_dir+f\"/SCORE/line_map_extractor/out/{scene_id}/{scene_id}_results_raw.npy\"\n",
    "### result saving path\n",
    "line_data_folder = root_dir+f\"/SCORE/line_map_extractor/out/{scene_id}/\" # numpy file with all the extracted 2D lines and merged 3D lines\n",
    "line_mesh_folder = root_dir+f\"/SCORE/line_map_extractor/out/{scene_id}/line_mesh_merged/\"\n",
    "for out_path in [line_data_folder, line_mesh_folder]:\n",
    "    if not os.path.exists(out_path):\n",
    "        os.makedirs(out_path)\n",
    "### load data\n",
    "scene_data = np.load(scene_data_path, allow_pickle=True).item()\n",
    "scene_pose = scene_data[\"scene_pose\"]\n",
    "scene_intrinsic = scene_data[\"scene_intrinsic\"]\n",
    "label_2_semantic_dict = scene_data[\"label_2_semantic_dict\"]\n",
    "scene_line_2D_end_points = scene_data[\"scene_line_2D_end_points\"]\n",
    "scene_line_2D_semantic_labels = scene_data[\"scene_line_2D_semantic_labels\"]\n",
    "scene_line_2D_params = scene_data[\"scene_line_2D_params\"]\n",
    "scene_line_2D_match_idx = scene_data[\"scene_line_2D_match_idx\"]\n",
    "scene_line_3D_end_points = scene_data[\"scene_line_3D_end_points\"]\n",
    "scene_line_3D_image_source = scene_data[\"scene_line_3D_image_source\"]\n",
    "scene_line_3D_semantic_labels = scene_data[\"scene_line_3D_semantic_labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### preprocessing for 3D line merging\n",
    "# each 3D line is treated as a vertex on the graph\n",
    "# the edges are defined by the parallel and proximity conditions\n",
    "nnode = len(scene_line_3D_semantic_labels)\n",
    "# precomputing to acclearte the graph construction\n",
    "pi_list = np.array([(scene_line_3D_end_points[i][0]+scene_line_3D_end_points[i][1]).reshape(1, 3)/2 for i in range(nnode)])\n",
    "p_diff_list = np.array([(scene_line_3D_end_points[i][1]-scene_line_3D_end_points[i][0]).reshape(1, 3) for i in range(nnode)])\n",
    "vi_list = np.array([p_diff_list[i]/np.linalg.norm(p_diff_list[i]) for i in range(nnode)])\n",
    "project_null_list = np.eye(3) - np.einsum('ijk,ijl->ikl', vi_list, vi_list)\n",
    "print(\"constructing the consistent graph\")\n",
    "def find_neighbors(i):\n",
    "    edges_i = []\n",
    "    edges_j = []\n",
    "    if i % 1000 == 0:\n",
    "        print(\"finding neighbors in progress:\", i/nnode*100,\"%\")\n",
    "    cur_image_idices = [scene_line_3D_image_source[i]] # cur_image_idices stores the indices of image from which the 3D lines are extracted\n",
    "    for j in range(i + 1, nnode):\n",
    "        if scene_line_3D_image_source[j] not in cur_image_idices: # lines extracted from a same image should not be merged\n",
    "            if abs(np.dot(vi_list[i], vi_list[j].T)) >= helper.params_3D[\"parrallel_thresh_3D\"]: # parallel condition\n",
    "                if np.linalg.norm(np.dot(project_null_list[i], (pi_list[i] - pi_list[j]).T)) <= helper.params_3D[\"overlap_thresh_3D\"]: # proximity condition\n",
    "                    edges_i.append(i)\n",
    "                    edges_j.append(j)\n",
    "                    cur_image_idices.append(scene_line_3D_image_source[j])\n",
    "    return edges_i, edges_j\n",
    "# parallel processing each 3D line\n",
    "results = Parallel(n_jobs=helper.params_3D[\"thread_number\"])(delayed(find_neighbors)(i) for i in range(nnode))\n",
    "\n",
    "# # none parallel version\n",
    "# results = []\n",
    "# for i in tqdm(range(nnode)):\n",
    "#     result = find_neighbors(i)\n",
    "#     results.append(result)\n",
    "\n",
    "# organize the edges and store as an itermediate result\n",
    "edges_i = []\n",
    "edges_j = []\n",
    "for edges_i_, edges_j_ in results:\n",
    "    edges_i.extend(edges_i_)\n",
    "    edges_j.extend(edges_j_)\n",
    "np.save(line_data_folder+scene_id+\"_edges.npy\",\n",
    "        {\"edges_i\": edges_i, \"edges_j\": edges_j})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the saved edges\n",
    "edge_data = np.load(line_data_folder+scene_id+\"_edges.npy\", allow_pickle=True).item()\n",
    "edges_i = np.array(edge_data[\"edges_i\"])\n",
    "edges_j = np.array(edge_data[\"edges_j\"])\n",
    "nnode = len(scene_line_3D_semantic_labels)\n",
    "# defines the data structure to be stored\n",
    "merged_semantic_label_3D =[]\n",
    "merged_scene_line_3D_end_points=[]\n",
    "\n",
    "#################### starting merging 3D lines based on graph connectivity ####################\n",
    "print(\"# 3D lines before merging\",nnode)\n",
    "mapping = list(range(nnode)) # this list records 3D line merging info\n",
    "edges_i_ = np.concatenate((edges_i,np.array(range(0,nnode))))\n",
    "edges_j_ = np.concatenate((edges_j,np.array(range(0,nnode))))\n",
    "vertex_concat = np.concatenate((edges_i_,edges_j_)) # concatenate the vertices for all th edges\n",
    "\n",
    "### step 0: remove suspicious lines which are observed only by a small number of images.\n",
    "unique_elements, counts = np.unique(vertex_concat, return_counts=True)\n",
    "print(helper.params_3D[\"degree_threshold\"]+2)\n",
    "vertex_deleted = unique_elements[counts<helper.params_3D[\"degree_threshold\"]+2] # plus 2: count the edge (vi,vi) \n",
    "for ver in vertex_deleted:\n",
    "    mapping[ver]=np.nan\n",
    "index_deleted = []\n",
    "for i in range(len(edges_i_)):\n",
    "    if edges_i_[i] in vertex_deleted or edges_j_[i] in vertex_deleted:\n",
    "        index_deleted.append(i)\n",
    "edges_i_=np.delete(edges_i_,index_deleted)\n",
    "edges_j_=np.delete(edges_j_,index_deleted)\n",
    "\n",
    "### step 1: iteratively find the vertex with largest degree, merge all its neighbors.  \n",
    "countt = 0\n",
    "while len(edges_i_)>0:  \n",
    "    vertex_concat = np.concatenate((edges_i_,edges_j_))\n",
    "    mode_result = stats.mode(vertex_concat)\n",
    "    # if mode_result.count<helper.params_3D[\"degree_threshold\"]+2: \n",
    "    ## if the remained lines is not observed by a enough number of images, \n",
    "    ## we assume it is a background line and discard it\n",
    "    #     left_vertex = np.unique(vertex_concat)\n",
    "    #     for ver in left_vertex:\n",
    "    #         mapping[ver]=np.nan\n",
    "    #     break\n",
    "    most_frequent_index = mode_result.mode\n",
    "    index_1 = np.where(edges_i_==most_frequent_index)\n",
    "    index_2 = np.where(edges_j_==most_frequent_index)\n",
    "    neighbors = np.unique(np.concatenate((edges_j_[index_1], edges_i_[index_2])))  # note that the neighbors contain itself \n",
    "    # delete neighbor vertex nodes and remove the edges\n",
    "    for neighbor in neighbors:\n",
    "        index_1 = np.where(edges_i_==neighbor)\n",
    "        index_2 = np.where(edges_j_==neighbor)\n",
    "        index_delete_neighbor = np.unique(np.concatenate((index_1[0], index_2[0])))\n",
    "        edges_i_=np.delete(edges_i_,index_delete_neighbor)\n",
    "        edges_j_=np.delete(edges_j_,index_delete_neighbor)\n",
    "    # update the end points\n",
    "    end_points = scene_line_3D_end_points[most_frequent_index]\n",
    "    v = end_points[1] - end_points[0]\n",
    "    v = v/np.linalg.norm(v)\n",
    "    sig_dim = np.argmax(np.abs(v))\n",
    "    for neighbor in neighbors:\n",
    "        end_points_temp = scene_line_3D_end_points[neighbor]\n",
    "        if end_points_temp[0][sig_dim]<end_points[0][sig_dim]:\n",
    "            end_points[0] = end_points[0] + (end_points_temp[0][sig_dim] - end_points[0][sig_dim])*(v/v[sig_dim]) \n",
    "        if end_points_temp[1][sig_dim]>end_points[1][sig_dim]:\n",
    "            end_points[1] = end_points[1] + (end_points_temp[1][sig_dim] - end_points[1][sig_dim])*(v/v[sig_dim]) \n",
    "    # For each unqiue semantic label, we create a 3D line in the map (with same geometric parameters)  \n",
    "    # Update the mapping list at the same time\n",
    "    cluster_semantic_labels = []\n",
    "    for neighbor in neighbors:\n",
    "        cluster_semantic_labels = np.append(cluster_semantic_labels,scene_line_3D_semantic_labels[neighbor])\n",
    "    unique_cluster_semantic_lables = np.unique(cluster_semantic_labels)\n",
    "    unique_cluster_semantic_lables = unique_cluster_semantic_lables[unique_cluster_semantic_lables!=0]\n",
    "    for label in unique_cluster_semantic_lables:\n",
    "        merged_semantic_label_3D.append(label)\n",
    "        merged_scene_line_3D_end_points.append(end_points)\n",
    "        for neighbor in neighbors:\n",
    "            if label == scene_line_3D_semantic_labels[neighbor]:\n",
    "                mapping[neighbor]=len(merged_semantic_label_3D)-1  # from the original index to the new index\n",
    "                \n",
    "\n",
    "        \n",
    "    # Debug: output the 3D line with more than 3 semantic labels\n",
    "    if len(unique_cluster_semantic_lables)>3: \n",
    "        point_diff = end_points[1] - end_points[0]\n",
    "        point_sets = []\n",
    "        for sample in range(300):\n",
    "            point_sets.append(end_points[0] + point_diff*sample/299)\n",
    "        pcd = o3d.geometry.PointCloud()\n",
    "        pcd.points = o3d.utility.Vector3DVector(point_sets)\n",
    "        o3d.io.write_point_cloud(line_mesh_folder + f\"multiple_semantic_{countt}.ply\", pcd)\n",
    "        countt+=1\n",
    "        for k in range(0,len(unique_cluster_semantic_lables)):\n",
    "            print(f\"{label_2_semantic_dict[unique_cluster_semantic_lables[k]]},\",end=\"\")\n",
    "\n",
    "#\n",
    "print(\"# 3D lines after merging:\",len(merged_scene_line_3D_end_points))\n",
    "\n",
    "### Step 2. Update projection error after merging 3D lines\n",
    "print(\"Updating projection error after merging\")\n",
    "scene_projection_error_r = {}\n",
    "scene_projection_error_t = {}\n",
    "scene_line_2D_match_idx_updated = {}\n",
    "for basename in scene_line_2D_match_idx.keys():\n",
    "    # if basename == 'frame_002180':\n",
    "    #     print(basename)\n",
    "    projection_error_r = []\n",
    "    projection_error_t = []\n",
    "    intrinsic = scene_intrinsic[basename]\n",
    "    pose_matrix = np.array(scene_pose[basename])\n",
    "    line_2D_match_idx = np.array(scene_line_2D_match_idx[basename])\n",
    "    line_2D_match_idx_updated = line_2D_match_idx.copy()\n",
    "    for j in range(len(line_2D_match_idx)):\n",
    "        if np.isnan(line_2D_match_idx[j]) or np.isnan(mapping[line_2D_match_idx[j]]): # no matched line\n",
    "            # print(line_2D_match_idx[j], mapping[line_2D_match_idx[j]])\n",
    "            line_2D_match_idx_updated[j] = -1\n",
    "            projection_error_r.append(np.nan)\n",
    "            projection_error_t.append(np.nan)\n",
    "        else:\n",
    "            mapping_idx = mapping[line_2D_match_idx[j]]\n",
    "            line_2D_match_idx_updated[j] = mapping_idx\n",
    "            n_j = scene_line_2D_params[basename][j].reshape(1,3)\n",
    "            end_points_3D = merged_scene_line_3D_end_points[mapping_idx]\n",
    "            v = end_points_3D[1] - end_points_3D[0]\n",
    "            v = v/np.linalg.norm(v)\n",
    "            # compute the projection error\n",
    "            error_rot,error_trans = helper.calculate_error(n_j,v,intrinsic,pose_matrix,end_points_3D[0],end_points_3D[1])\n",
    "            projection_error_r.append(np.abs(error_rot))\n",
    "            projection_error_t.append(np.abs(error_trans)) \n",
    "            if np.abs(error_trans) > 0.1:\n",
    "                print(f\"Warning: projection error too large for {basename} at index {j}, error_t={error_trans}\")\n",
    "    scene_line_2D_match_idx_updated[basename] = line_2D_match_idx_updated          \n",
    "    scene_projection_error_r[basename] = projection_error_r\n",
    "    scene_projection_error_t[basename] = projection_error_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### save data for relocalization experiements\n",
    "np.save(line_data_folder+scene_id+\"_results_merged.npy\", {\n",
    "    \"scene_pose\": scene_pose,\n",
    "    \"scene_intrinsic\": scene_intrinsic,\n",
    "    \"label_2_semantic_dict\": label_2_semantic_dict,\n",
    "    ###\n",
    "    \"scene_line_2D_semantic_labels\": scene_line_2D_semantic_labels,\n",
    "    \"scene_line_2D_params\": scene_line_2D_params,\n",
    "    \"scene_line_2D_end_points\": scene_line_2D_end_points,\n",
    "    \"scene_line_2D_match_idx_updated\": scene_line_2D_match_idx_updated,\n",
    "    \"scene_projection_error_r\": scene_projection_error_r,\n",
    "    \"scene_projection_error_t\": scene_projection_error_t,\n",
    "    ###\n",
    "    \"merged_scene_line_3D_semantic_labels\": merged_semantic_label_3D,\n",
    "    \"merged_scene_line_3D_end_points\": merged_scene_line_3D_end_points,\n",
    "    ###\n",
    "    \"params_3D\": helper.params_3D\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### output 3D line mesh for visualization \n",
    "# save all merged 3D lines\n",
    "point_sets=[]\n",
    "for i in range(len(merged_semantic_label_3D)):\n",
    "    end_points = merged_scene_line_3D_end_points[i]\n",
    "    point_diff = end_points[1] - end_points[0]\n",
    "    for sample in range(300):\n",
    "        point_sets.append(end_points[0] + point_diff*sample/299)\n",
    "point_sets = np.vstack(point_sets)\n",
    "pcd = o3d.geometry.PointCloud()\n",
    "pcd.points = o3d.utility.Vector3dVector(point_sets)\n",
    "o3d.io.write_point_cloud(line_mesh_folder+scene_id+f\"_merged_3D_line_mesh.ply\", pcd)\n",
    "\n",
    "# save the 3D line mesh for each semantic label\n",
    "def process_label(i, semantic_label):\n",
    "    if int(semantic_label) == 0:\n",
    "        return\n",
    "    index = np.where(merged_semantic_label_3D == semantic_label)\n",
    "    print(\"semantic label:\" + f\"{label_2_semantic_dict[int(semantic_label)]}\" + \" number of lines:\", len(index[0]))\n",
    "    point_sets = []\n",
    "    for j in range(len(index[0])):\n",
    "        end_points = merged_scene_line_3D_end_points[i]\n",
    "        point_diff = end_points[1] - end_points[0]\n",
    "        for sample in range(300):\n",
    "            point_sets.append(end_points[0] + point_diff*sample/299)\n",
    "    if point_sets:\n",
    "        point_sets = np.vstack(point_sets)\n",
    "        pcd = o3d.geometry.PointCloud()\n",
    "        pcd.points = o3d.utility.Vector3dVector(point_sets)\n",
    "        o3d.io.write_point_cloud(line_mesh_folder + f\"{label_2_semantic_dict[int(semantic_label)]}.ply\", pcd)\n",
    "semantic_labels_all = np.unique(merged_semantic_label_3D)\n",
    "# Parallel(n_jobs=params_3D[\"thread_number\"])(delayed(process_label)(i, semantic_label) for i, semantic_label in enumerate(semantic_labels_all))\n",
    "Parallel(n_jobs=8)(delayed(process_label)(i, semantic_label) for i, semantic_label in enumerate(semantic_labels_all))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "elsed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
